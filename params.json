{"name":"Paul's Presumptions Python Pages","tagline":"My useful programming pages geared towards scientific programming","body":"# How to do embarrassingly parallel processing in python across multiple cores and multiple computers\r\n\r\n## What types of code is this good for?\r\n\r\nThis sort of code is good for running embarrassingly parallel code.  Embarrassingly parallel code is where the the results from one calculation don't effect the result from other calculations.  \r\n\r\nExample of embarrassingly parallel code:\r\n\r\n```python\r\nimport numpy as np\r\nx=np.arange(0,2*pi, 0.01)\r\nn=len(x)\r\ny=np.zeros(n)\r\n\r\nfor i in range(n):\r\n  y[i]=np.sin(x[i])\r\n````\r\n\r\n`sin(x[i])` doesn't care what `sin(x[i-1])` is.  So you could compute the values in any order.  Now this is a bad example since `sin` is not terribly computational intense compared to the overhead of parallel processing, but it gives you an idea of the sort of thing I'll be telling you python can trivially parallelize.\r\n\r\n## Setup (1 computer)\r\nWhat you need:\r\n*  Ipython (that's it!)\r\n\r\nInstructions:\r\n* at the command line type:\r\n\r\n```ipcluster start --n=4```\r\n\r\nThis sets up a cluster of processes for Ipython.  `n` is the number of processes to start, which should be equal to the number of cores you want to use.  \r\n\r\n* Open another terminal and start and ipython (or ipython+pylab) session.\r\n\r\n## How to run code in parallel (same for both single machine and multiple machines!!)\r\n\r\nSo to work this you need to set up your function to use python's map function.  The non-parallel way of doing it (again, using the sin example from above):\r\n\r\n```y=map(np.sin,x)```\r\n\r\nNote that you can have more than one variable to iterate over with map, but for simplicity's sake I'm just dealing with one variable here.  Now for the parallel version:\r\n\r\n```python\r\nimport numpy as np\r\nfrom IPython.parallel import Client\r\nrc=Client()\r\ndview=rc[:]\r\ndview.execute('import numpy as np')\r\nlview=rc.load_balanced_view()\r\nlview.block=True\r\ny=lview.map(np.sin,x)\r\n```\r\n\r\nThat's it!! Now for a little bit of an explanation what's going on.  `Client()` is a client to your ipcluster that you already have running.  `dview` is a direct view, which is a direct connection to each of the processes. `dview` also lets you directly tell all the processes to do stuff, like say load the numpy library, using the `execute` function. `lview` is a load balanced view.  The difference between the two is that `dview` will take n chunks of data and divide them over m processes (i.e. each process gets n/m threads), but `lview` accounts for the fact that not every iteration will take the same amount of time to compute and balances the computation between processes.  This is especially important if you are running over multiple computers where the processors may have different speeds.\r\n\r\nNow say that you have a god awful hideously computationally expensive function that you've already optimized by putting it into c code and wrapping it using cython (this is what I'm using it for) and it has a whole bunch of parameters other than the one we're iterating over.  The above still works with minimal changes:\r\n\r\nHere's my example function:\r\n```python\r\ndef myfxn(a,b,c,d):\r\n  ....\r\n  return z\r\n```\r\n\r\nHere's my code to run it:\r\n```python\r\nimport mylib\r\n\r\n#set up parallel processes, start ipcluster from command line prior!\r\nfrom IPython.parallel import Client\r\nrc=Client()\r\ndview=rc[:]\r\n\r\n#...do stuff to get iterable_of_a and b,c,d....\r\n\r\nmylamfxn=lambda a:mylib.myfxn(a,b,c,d)\r\n    \r\ndview.execute('import mylib')\r\nmydict=dict(b=b, c=c, d=d)\r\ndview.push(mydict)\r\nlview=rc.load_balanced_view()\r\nlview.block=True\r\ny=lview.map(mylamfxn,iterable_of_a)\r\n```\r\n\r\nSo there's two new things go on here.  `lambda` is a built in python thingy that you can use to make a function of multiple parameters into a function of a single parameter setting constants to all of the non-iterable parameters.  The `push()` function on the directview lets you send data from your main program to the various nodes on the ipcluster.  In this case, we're passing along the other parameters needed by `mylamfxn`.  Any this works, but there are some important caveats.\r\n\r\n### Caveat Programmor\r\n\r\nYou have to be really careful about your working directory and `$PYTHONPATH`.  `mylib` needs to be on the `$PYTHONPATH` for all of the ipython processes, or else there will be an error.  Also, unless you set it specifically, the working directory for ipcluster will be the directory you started the process in, which can be important if your hideous function needs to find particular files (like mine did).\r\n\r\nHaving said that, there is literally no change to your python program between running on a computer with multiples cores and running across cores on multiple computers.  The only changes are in worrying about setting up your cluster properly, which I detail below.\r\n\r\n## What you need to get started (multiple computers)\r\n* IP address of the host computer\r\n* Know where the `$IPYTHONPATH` is on each computer\r\n* All computers in the cluster can see the host computer\r\n* Understand how python working directory and `$PYTHONPATH` affect how your code works\r\n**Note**:  I don't know if this will work across different architectures.  My Mac wouldn't play nice with three computers running Linux and I haven't had a chance to troubleshoot it....\r\n\r\n### Step-by-step instructions:\r\n* If you did my example above with a local cluster, kill it with CTRL-C\r\n* run the command `ipcontroller --ip=[host ip here]` on the computer you want to actually run your main python program\r\nThis starts the controller process.  Your previous ipcluster command did this automagically, but you're doing it differently this time.  \r\n* Open a new terminal on the local computer.  Run the command `ipcluster engines --n=[number of cores in machine]`\r\n* Do the following on each of the cluster computers\r\n  * Copy `$IPYTHONDIR/profile_default/security/ipcontroller-engine.json` from the host to the same directory the cluster computer.  This has the authorization needed to talk to the controller as well as the ip the controller is listening on\r\n  * run `ipcluster engine --n=[number of cores in machine]`\r\n  * Every time you do this, you should get a message on the host machine terminal running `ipcontroller` saying the new cores are registered\r\n* compute away! but bear in mind the caveats regarding working directories and that there's overhead of talking between the cluster nodes and the main host\r\n\r\n###Caveats for clusters\r\n\r\nThe above has security issues.  These can be fixed via ssh, but that will slow things down and is something I haven't had time to play with.  Firewalls are your friend.\r\n","google":"UA-43010472-1","note":"Don't delete this file! It's used internally to help with page regeneration."}